% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/2-2-dnnet.R
\name{dnnet}
\alias{dnnet}
\title{Multilayer Perceptron Model for Regression or Classification}
\usage{
dnnet(train, validate = NULL, norm.x = TRUE,
  norm.y = ifelse(is.factor(train@y), FALSE, TRUE), activate = "elu",
  n.hidden = c(10, 10), learning.rate = ifelse(learning.rate.adaptive \%in\%
  c("adam"), 0.001, 0.01), l1.reg = 0, l2.reg = 0, n.batch = 100,
  n.epoch = 100, early.stop = ifelse(is.null(validate), FALSE, TRUE),
  early.stop.det = 5, plot = FALSE, accel = c("rcpp", "gpu", "none")[3],
  learning.rate.adaptive = c("constant", "adadelta", "adagrad", "momentum",
  "adam")[2], rho = c(0.9, 0.95, 0.99, 0.999)[ifelse(learning.rate.adaptive ==
  "momentum", 1, 3)], epsilon = c(10^-10, 10^-8, 10^-6, 10^-4)[2],
  beta1 = 0.9, beta2 = 0.999, loss.f = ifelse(is.factor(train@y), "logit",
  "mse"), ...)
}
\arguments{
\item{train}{A \code{dnnetInput} object, the training set.}

\item{validate}{A \code{dnnetInput} object, the validation set, optional.}

\item{norm.x}{A boolean variable indicating whether to normalize the input matrix.}

\item{norm.y}{A boolean variable indicating whether to normalize the response (if continuous).}

\item{activate}{Activation Function. One of the following,
"sigmoid", "tanh", "relu", "prelu", "elu", "celu".}

\item{learning.rate}{Initial learning rate, 0.001 by default; If "adam" is chosen as
an adaptive learning rate adjustment method, 0.1 by defalut.}

\item{l1.reg}{weight for l1 regularization, optional.}

\item{l2.reg}{weight for l2 regularization, optional.}

\item{n.batch}{Batch size for batch gradient descent.}

\item{n.epoch}{Maximum number of epochs.}

\item{early.stop}{Indicate whether early stop is used (only if there exists a validation set).}

\item{early.stop.det}{Number of epochs of increasing loss to determine the early stop.}

\item{plot}{Indicate whether to plot the loss.}

\item{accel}{"rcpp" to use the Rcpp version and "none" (default) to use the R version for back propagation.}

\item{learning.rate.adaptive}{Adaptive learning rate adjustment methods, one of the following,
"constant", "adadelta", "adagrad", "momentum", "adam".}

\item{epsilon}{A parameter used in Adagrad and Adam.}

\item{beta1}{A parameter used in Adam.}

\item{beta2}{A parameter used in Adam.}

\item{loss.f}{Loss function of choice.}
}
\value{
Returns a \code{DnnModelObj} object.
}
\description{
Fit a Multilayer Perceptron Model for Regression or Classification
}
\seealso{
\code{\link{dnnet-class}}\cr
\code{\link{dnnetInput-class}}\cr
\code{\link{actF}}
}
