% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/2-4-activate.R
\name{actF}
\alias{actF}
\alias{sigmoid}
\alias{sigmoid_}
\alias{tanh}
\alias{tanh_}
\alias{relu}
\alias{relu_}
\alias{prelu}
\alias{prelu_}
\alias{elu}
\alias{elu_}
\alias{celu}
\alias{celu_}
\title{Activation functions and their first order derivatives}
\usage{
sigmoid(x)

sigmoid_(x)

tanh(x)

tanh_(x)

relu(x)

relu_(x)

prelu(x, a = 0.2)

prelu_(x, a = 0.2)

elu(x, a = 1)

elu_(x, a = 1)

celu(x, a = 1)

celu_(x, a = 1)
}
\arguments{
\item{x}{Input of the activation function}

\item{a}{a or alpha in the function}

\item{alpha}{A pre-specified numeric value less or equal to 1.}

\item{alpha}{A pre-specified numeric value less or equal to 1.}
}
\value{
Returns the value after activation
}
\description{
A collection of activation functions and their first
 order derivatives used in deep neural networks
}
\details{
Sigmoid Function:
sigmoid(x) = 1/(1+exp(-x))

Hyperbolic Tangent Function:
tanh(x) = (exp(x) - exp(-x))/(exp(x) + exp(-x))

Rectified Linear Units:
relu(x) = max(0, x)

Leaky ReLU:
prelu(x, a) = max(x*a, x), (a<1)

Exponential Linear Units:
elu(x, alpha) = max(alpha*(exp(x)-1), x), (alpha<=1)

Continuously Differentiable Exponential Linear Units
celu(x, alpha) = max(alpha*(exp(x/alpha)-1), x)
}
\section{Methods (by generic)}{

Sigmoid function.


First order derivative of Sigmoid function.


Tanh function.


First order derivative of tanh function.


ReLU.


First order derivative of ReLU.


Leaky ReLU.


First order derivative of leaky ReLU.


ELU.


First order derivative of ELU function.


CELU.


First order derivative of CELU function.
}

\seealso{
\code{\link{nn.regresser}}\cr
\code{\link{nn.classifier}}\cr
}
